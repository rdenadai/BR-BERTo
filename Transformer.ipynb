{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:34:09.690991Z",
     "start_time": "2020-07-23T17:34:09.688755Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sys.path.append(f\"{os.getcwd()}/../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:34:10.167330Z",
     "start_time": "2020-07-23T17:34:09.692109Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "vocab_size = 52_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:34:54.912531Z",
     "start_time": "2020-07-23T17:34:10.168521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/ssd/programas/WordEmbeddingPortugues/notebooks/../data/embedding/corpus.txt']\n"
     ]
    }
   ],
   "source": [
    "paths = [str(x) for x in Path(f\"{os.getcwd()}/../data/embedding\").glob(\"**/corpus.txt\")]\n",
    "print(paths)\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=vocab_size, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:34:55.439580Z",
     "start_time": "2020-07-23T17:34:54.913743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: não foi possível criar o diretório “BR_BERTo”: Arquivo existe\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['BR_BERTo/vocab.json', 'BR_BERTo/merges.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mkdir BR_BERTo\n",
    "# Save files to disk\n",
    "tokenizer.save_model(\"BR_BERTo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:34:55.551419Z",
     "start_time": "2020-07-23T17:34:55.440879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'gostei', 'Ġmuito', 'Ġdessa', 'Ġideia', '</s>']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./BR_BERTo/vocab.json\",\n",
    "    \"./BR_BERTo/merges.txt\",\n",
    ")\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "print(tokenizer.encode(\"gostei muito dessa ideia\".lower()).tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:34:56.058146Z",
     "start_time": "2020-07-23T17:34:55.552503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that PyTorch sees it\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:34:57.474856Z",
     "start_time": "2020-07-23T17:34:56.060461Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:35:00.746510Z",
     "start_time": "2020-07-23T17:34:57.476088Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:35:00.751520Z",
     "start_time": "2020-07-23T17:35:00.747694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84095008"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:35:00.929301Z",
     "start_time": "2020-07-23T17:35:00.753105Z"
    }
   },
   "outputs": [],
   "source": [
    "# from nlp import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"text\", data_files=[f\"{os.getcwd()}/../data/embedding/corpus.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:35:01.105594Z",
     "start_time": "2020-07-23T17:35:00.934058Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(dataset[\"train\"])\n",
    "# dataset[\"train\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:35:01.281643Z",
     "start_time": "2020-07-23T17:35:01.109876Z"
    }
   },
   "outputs": [],
   "source": [
    "# dt = pd.read_csv(f\"{os.getcwd()}/../data/embedding/colab.txt0001\", header=None, chunksize=1)\n",
    "# len(dt)\n",
    "# next(dt).to_numpy()[0][0]\n",
    "# next(dt).to_numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:35:01.571461Z",
     "start_time": "2020-07-23T17:35:01.286663Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlp import load_dataset\n",
    "from transformers import RobertaTokenizerFast\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def stream(file_path):\n",
    "    with open(file_path, encoding=\"utf-8\") as fh:\n",
    "        for line in fh.readlines():\n",
    "            yield line.strip()\n",
    "\n",
    "class EsperantoDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path: str):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.file_path = file_path\n",
    "        self.dataset = pd.read_csv(self.file_path, header=None, iterator=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 5258624 # fazer um wc -l para ver a qtde de linhas\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        batch_encoding = self.tokenizer(str(text).strip(), add_special_tokens=True, truncation=True, max_length=64)\n",
    "        return torch.tensor(batch_encoding[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        example = self.preprocess(self.dataset.get_chunk(1).to_numpy()[0][0])\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:35:01.774835Z",
     "start_time": "2020-07-23T17:35:01.572779Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./BR_BERTo\", max_len=512)\n",
    "dataset = EsperantoDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=f\"{os.getcwd()}/../data/embedding/corpus.txt\"\n",
    ")\n",
    "\n",
    "# from transformers import LineByLineTextDataset\n",
    "\n",
    "# dataset = LineByLineTextDataset(\n",
    "#     tokenizer=tokenizer,\n",
    "#     file_path=f\"{os.getcwd()}/../data/embedding/colab.txt0001\",\n",
    "#     block_size=32,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:35:01.947042Z",
     "start_time": "2020-07-23T17:35:01.775980Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:35:03.520961Z",
     "start_time": "2020-07-23T17:35:01.951819Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./BR_BERTo\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-23T17:34:09.567Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2c9d651f71407db9901baeaf1a1dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e477d6d955642d385b980e2e0a70d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=164332.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-23T17:34:09.568Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./BR_BERTo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-23T17:34:09.569Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./BR_BERTo\",\n",
    "    tokenizer=\"./BR_BERTo\", topk=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-23T17:34:09.570Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_mask(\"eu gosto muito de <mask>\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
